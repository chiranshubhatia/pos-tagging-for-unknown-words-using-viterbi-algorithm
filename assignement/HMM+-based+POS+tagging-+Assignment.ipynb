{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint,time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the list of sents to list of words(word,pos tuples)\n",
    "tagged_words=[tup for sent in nltk_data for tup in sent]\n",
    "print(len(tagged_words))\n",
    "tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# splitting into train and test set\n",
    "random.seed(1234)\n",
    "train_set,test_set=train_test_split(nltk_data,test_size=0.05)\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting list of train tagged words\n",
    "train_tagged_words=[tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens\n",
    "tokens=[pair[0] for pair in train_tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary\n",
    "V=set(tokens)\n",
    "print(len(V))\n",
    "#number of tags\n",
    "T=set([pair[1] for pair in train_tagged_words])\n",
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emission Probabilities\n",
    "#computing p(w/t) and storing in TxV matrix\n",
    "t=len(T)\n",
    "v=len(V)\n",
    "w_given_t=np.zeros((t,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_given_tag(word,tag,train_bag=train_tagged_words):\n",
    "    tag_list=[pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag=len(tag_list)\n",
    "    w_given_tag_list=[pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag=len(w_given_tag_list)\n",
    "    return (count_w_given_tag,count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition probabilities\n",
    "def t2_given_t1(t2,t1,train_bag=train_tagged_words):\n",
    "    tags=[pair[1] for pair in train_bag]\n",
    "    count_t1=len([t for t in tags if t==t1])\n",
    "    count_t2_t1=0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1]==t2:\n",
    "            count_t2_t1+=1\n",
    "    return (count_t2_t1,count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating txt transition matrix of tags\n",
    "# each column is t2 , each row is t1\n",
    "tags_matrix=np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADV</th>\n",
       "      <th>VERB</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>PRT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.092545</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>0.027455</td>\n",
       "      <td>0.067273</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.052727</td>\n",
       "      <td>0.089091</td>\n",
       "      <td>0.174273</td>\n",
       "      <td>0.091273</td>\n",
       "      <td>0.044455</td>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.002455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.119684</td>\n",
       "      <td>0.184083</td>\n",
       "      <td>0.209903</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.353888</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.035237</td>\n",
       "      <td>0.032807</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>0.024909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.163706</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.074252</td>\n",
       "      <td>0.056009</td>\n",
       "      <td>0.061130</td>\n",
       "      <td>0.026244</td>\n",
       "      <td>0.204193</td>\n",
       "      <td>0.055529</td>\n",
       "      <td>0.141783</td>\n",
       "      <td>0.017123</td>\n",
       "      <td>0.009762</td>\n",
       "      <td>0.187390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.039847</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.093103</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>0.483525</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>0.073946</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.237743</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.029599</td>\n",
       "      <td>0.004647</td>\n",
       "      <td>0.264781</td>\n",
       "      <td>0.017123</td>\n",
       "      <td>0.147227</td>\n",
       "      <td>0.013135</td>\n",
       "      <td>0.177996</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.042624</td>\n",
       "      <td>0.043978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.135198</td>\n",
       "      <td>0.030636</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>0.031968</td>\n",
       "      <td>0.078921</td>\n",
       "      <td>0.349317</td>\n",
       "      <td>0.068265</td>\n",
       "      <td>0.117216</td>\n",
       "      <td>0.129204</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.014652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.034767</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.217547</td>\n",
       "      <td>0.035545</td>\n",
       "      <td>0.110523</td>\n",
       "      <td>0.081434</td>\n",
       "      <td>0.168469</td>\n",
       "      <td>0.135490</td>\n",
       "      <td>0.091545</td>\n",
       "      <td>0.065178</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>0.031267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.017825</td>\n",
       "      <td>0.021705</td>\n",
       "      <td>0.044744</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.640718</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>0.038559</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.205044</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.039195</td>\n",
       "      <td>0.063290</td>\n",
       "      <td>0.034376</td>\n",
       "      <td>0.069287</td>\n",
       "      <td>0.325016</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>0.008246</td>\n",
       "      <td>0.321161</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>0.106340</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.064340</td>\n",
       "      <td>0.020179</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.698975</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.012239</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>0.078234</td>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>0.011247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.035965</td>\n",
       "      <td>0.041569</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.059318</td>\n",
       "      <td>0.352172</td>\n",
       "      <td>0.054180</td>\n",
       "      <td>0.156936</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.053246</td>\n",
       "      <td>0.113965</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.005138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.042498</td>\n",
       "      <td>0.053939</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>0.245178</td>\n",
       "      <td>0.010461</td>\n",
       "      <td>0.404054</td>\n",
       "      <td>0.100686</td>\n",
       "      <td>0.019941</td>\n",
       "      <td>0.086303</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .       NUM         X      PRON      NOUN       ADV      VERB  \\\n",
       ".     0.092545  0.079000  0.027455  0.067273  0.221000  0.052727  0.089091   \n",
       "NUM   0.119684  0.184083  0.209903  0.001519  0.353888  0.003038  0.017922   \n",
       "X     0.163706  0.002880  0.074252  0.056009  0.061130  0.026244  0.204193   \n",
       "PRON  0.039847  0.007663  0.093103  0.008046  0.211111  0.035249  0.483525   \n",
       "NOUN  0.237743  0.009037  0.029599  0.004647  0.264781  0.017123  0.147227   \n",
       "ADV   0.135198  0.030636  0.022977  0.014985  0.031968  0.078921  0.349317   \n",
       "VERB  0.034767  0.022867  0.217547  0.035545  0.110523  0.081434  0.168469   \n",
       "DET   0.017825  0.021705  0.044744  0.003516  0.640718  0.012368  0.038559   \n",
       "ADP   0.039195  0.063290  0.034376  0.069287  0.325016  0.013493  0.008246   \n",
       "ADJ   0.064340  0.020179  0.020675  0.000662  0.698975  0.004797  0.012239   \n",
       "CONJ  0.035965  0.041569  0.007940  0.059318  0.352172  0.054180  0.156936   \n",
       "PRT   0.042498  0.053939  0.013730  0.018960  0.245178  0.010461  0.404054   \n",
       "\n",
       "           DET       ADP       ADJ      CONJ       PRT  \n",
       ".     0.174273  0.091273  0.044455  0.058364  0.002455  \n",
       "NUM   0.003341  0.035237  0.032807  0.013670  0.024909  \n",
       "X     0.055529  0.141783  0.017123  0.009762  0.187390  \n",
       "PRON  0.009962  0.021073  0.073946  0.004981  0.011494  \n",
       "NOUN  0.013135  0.177996  0.012110  0.042624  0.043978  \n",
       "ADV   0.068265  0.117216  0.129204  0.006660  0.014652  \n",
       "VERB  0.135490  0.091545  0.065178  0.005367  0.031267  \n",
       "DET   0.005699  0.009094  0.205044  0.000485  0.000243  \n",
       "ADP   0.321161  0.017134  0.106340  0.000964  0.001499  \n",
       "ADJ   0.005127  0.078234  0.066159  0.017367  0.011247  \n",
       "CONJ  0.119103  0.053246  0.113965  0.000467  0.005138  \n",
       "PRT   0.100686  0.019941  0.086303  0.002288  0.001961  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the matrix to a df for better readability\n",
    "tags_df=pd.DataFrame(tags_matrix,columns=list(T),index=list(T))\n",
    "tags_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1] \n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DET'), ('Tokyo', 'NOUN'), ('Stock', 'NOUN'), ('Price', 'NOUN'), ('Index', 'NOUN'), ('-LRB-', '.'), ('Topix', 'NOUN'), ('-RRB-', '.'), ('of', 'ADP'), ('all', 'DET'), ('issues', 'NOUN'), ('listed', 'VERB'), ('*', 'X'), ('in', 'ADP'), ('the', 'DET'), ('First', 'NOUN'), ('Section', 'NOUN'), (',', '.'), ('which', 'DET'), ('*T*-1', 'X'), ('gained', 'VERB'), ('16.05', 'NUM'), ('points', 'NOUN'), ('Tuesday', 'NOUN'), (',', '.'), ('was', 'VERB'), ('down', 'ADV'), ('1.46', 'NUM'), ('points', 'NOUN'), (',', '.'), ('or', 'CONJ'), ('0.05', 'NUM'), ('%', 'NOUN'), (',', '.'), ('at', 'ADP'), ('2691.19', 'NUM'), ('.', '.')], [('``', '.'), ('It', 'PRON'), (\"'s\", 'VERB'), ('a', 'DET'), ('cosmetic', 'ADJ'), ('move', 'NOUN'), (',', '.'), (\"''\", '.'), ('said', 'VERB'), ('*T*-1', 'X'), ('Jonathan', 'NOUN'), ('S.', 'NOUN'), ('Gelles', 'NOUN'), ('of', 'ADP'), ('Wertheim', 'NOUN'), ('Schroder', 'NOUN'), ('&', 'CONJ'), ('Co', 'NOUN'), ('.', '.')], [('The', 'DET'), ('flow', 'NOUN'), ('of', 'ADP'), ('Japanese', 'ADJ'), ('funds', 'NOUN'), ('has', 'VERB'), ('set', 'VERB'), ('in', 'ADP'), ('motion', 'NOUN'), ('``', '.'), ('a', 'DET'), ('process', 'NOUN'), ('whereby', 'ADV'), ('these', 'DET'), ('economies', 'NOUN'), ('will', 'VERB'), ('be', 'VERB'), ('knitted', 'VERB'), ('*-1', 'X'), ('together', 'PRT'), ('by', 'ADP'), ('the', 'DET'), ('great', 'ADJ'), ('Japanese', 'ADJ'), ('investment', 'NOUN'), ('machine', 'NOUN'), ('*T*-2', 'X'), (',', '.'), (\"''\", '.'), ('says', 'VERB'), ('0', 'X'), ('*T*-3', 'X'), ('Robert', 'NOUN'), ('Hormats', 'NOUN'), (',', '.'), ('vice', 'NOUN'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Goldman', 'NOUN'), ('Sachs', 'NOUN'), ('International', 'NOUN'), ('Corp', 'NOUN'), ('.', '.')], [('Columbia', 'NOUN'), ('has', 'VERB'), ('only', 'ADV'), ('about', 'ADP'), ('10', 'NUM'), ('million', 'NUM'), ('common', 'ADJ'), ('shares', 'NOUN'), ('in', 'ADP'), ('public', 'ADJ'), ('hands', 'NOUN'), ('.', '.')], [('60', 'NUM'), ('million', 'NUM'), ('Swiss', 'ADJ'), ('francs', 'NOUN'), ('of', 'ADP'), ('privately', 'ADV'), ('placed', 'VERB'), ('convertible', 'ADJ'), ('notes', 'NOUN'), ('due', 'ADJ'), ('Dec.', 'NOUN'), ('31', 'NUM'), (',', '.'), ('1993', 'NUM'), (',', '.'), ('with', 'ADP'), ('a', 'DET'), ('fixed', 'VERB'), ('0.25', 'NUM'), ('%', 'NOUN'), ('coupon', 'NOUN'), ('at', 'ADP'), ('par', 'NOUN'), ('via', 'ADP'), ('Union', 'NOUN'), ('Bank', 'NOUN'), ('of', 'ADP'), ('Switzerland', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 5 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(5)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "print(test_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  32.10798645019531\n",
      "[('The', 'DET'), ('Tokyo', 'NOUN'), ('Stock', 'NOUN'), ('Price', 'NOUN'), ('Index', 'NOUN'), ('-LRB-', '.'), ('Topix', '.'), ('-RRB-', '.'), ('of', 'ADP'), ('all', 'DET'), ('issues', 'NOUN'), ('listed', 'VERB'), ('*', 'X'), ('in', 'ADP'), ('the', 'DET'), ('First', 'NOUN'), ('Section', 'NOUN'), (',', '.'), ('which', 'DET'), ('*T*-1', 'X'), ('gained', 'VERB'), ('16.05', '.'), ('points', 'NOUN'), ('Tuesday', 'NOUN'), (',', '.'), ('was', 'VERB'), ('down', 'ADV'), ('1.46', '.'), ('points', 'NOUN'), (',', '.'), ('or', 'CONJ'), ('0.05', '.'), ('%', 'NOUN'), (',', '.'), ('at', 'ADP'), ('2691.19', '.'), ('.', '.'), ('``', '.'), ('It', 'PRON'), (\"'s\", 'VERB'), ('a', 'DET'), ('cosmetic', 'NOUN'), ('move', 'NOUN'), (',', '.'), (\"''\", '.'), ('said', 'VERB'), ('*T*-1', 'X'), ('Jonathan', 'NOUN'), ('S.', 'NOUN'), ('Gelles', '.'), ('of', 'ADP'), ('Wertheim', '.'), ('Schroder', '.'), ('&', 'CONJ'), ('Co', 'NOUN'), ('.', '.'), ('The', 'DET'), ('flow', 'NOUN'), ('of', 'ADP'), ('Japanese', 'ADJ'), ('funds', 'NOUN'), ('has', 'VERB'), ('set', 'VERB'), ('in', 'ADP'), ('motion', 'NOUN'), ('``', '.'), ('a', 'DET'), ('process', 'NOUN'), ('whereby', '.'), ('these', 'DET'), ('economies', 'NOUN'), ('will', 'VERB'), ('be', 'VERB'), ('knitted', '.'), ('*-1', 'X'), ('together', 'PRT'), ('by', 'ADP'), ('the', 'DET'), ('great', 'ADJ'), ('Japanese', 'ADJ'), ('investment', 'NOUN'), ('machine', 'NOUN'), ('*T*-2', 'X'), (',', '.'), (\"''\", '.'), ('says', 'VERB'), ('0', 'X'), ('*T*-3', 'X'), ('Robert', 'NOUN'), ('Hormats', 'NOUN'), (',', '.'), ('vice', 'NOUN'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Goldman', 'NOUN'), ('Sachs', 'NOUN'), ('International', 'NOUN'), ('Corp', 'NOUN'), ('.', '.'), ('Columbia', 'NOUN'), ('has', 'VERB'), ('only', 'ADV'), ('about', 'ADP'), ('10', 'NUM'), ('million', 'NUM'), ('common', 'ADJ'), ('shares', 'NOUN'), ('in', 'ADP'), ('public', 'ADJ'), ('hands', 'NOUN'), ('.', '.'), ('60', 'NUM'), ('million', 'NUM'), ('Swiss', 'ADJ'), ('francs', 'NOUN'), ('of', 'ADP'), ('privately', 'ADV'), ('placed', 'VERB'), ('convertible', 'ADJ'), ('notes', 'NOUN'), ('due', 'ADJ'), ('Dec.', 'NOUN'), ('31', 'NUM'), (',', '.'), ('1993', 'NUM'), (',', '.'), ('with', 'ADP'), ('a', 'DET'), ('fixed', 'VERB'), ('0.25', 'NUM'), ('%', 'NOUN'), ('coupon', 'NOUN'), ('at', 'ADP'), ('par', 'NOUN'), ('via', 'ADP'), ('Union', 'NOUN'), ('Bank', 'NOUN'), ('of', 'ADP'), ('Switzerland', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "print(tagged_seq)\n",
    "#print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214285714285714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "\n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading smaple test file\n",
    "f=open('Test_sentences.txt','r')\n",
    "sentences=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing sentences to vanila viterbi algo\n",
    "words =word_tokenize(sentences)\n",
    "start=time.time()\n",
    "tagged_sample_test_1=Viterbi(words)\n",
    "end=time.time()\n",
    "difference=end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', '.'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', '.'), ('.', '.'), ('Android', '.'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', '.'), ('worldwide', '.'), ('on', 'ADP'), ('smartphones', '.'), ('since', 'ADP'), ('2011', '.'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', '.'), ('.', '.'), ('Google', '.'), ('and', 'CONJ'), ('Twitter', '.'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', '.'), ('that', 'DET'), ('gave', 'VERB'), ('Google', '.'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', '.'), (\"'s\", 'VERB'), ('firehose', '.'), ('.', '.'), ('Twitter', '.'), ('is', 'VERB'), ('an', 'DET'), ('online', '.'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', '.'), ('with', 'ADP'), ('messages', '.'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', '.'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', '.'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', '.'), ('.', '.'), ('The', 'DET'), ('2018', '.'), ('FIFA', '.'), ('World', 'NOUN'), ('Cup', '.'), ('is', 'VERB'), ('the', 'DET'), ('21st', '.'), ('FIFA', '.'), ('World', 'NOUN'), ('Cup', '.'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', '.'), ('contested', '.'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', '.'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', '.'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', '.'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', '.'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', '.'), ('invited', '.'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', '.'), ('Satellite', '.'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sample_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can see that Android , OS , Google  and many more are tagged incorrectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique tags are there in the corpus\n",
    "tags=[pair[1] for pair in tagged_words]\n",
    "unique_tags=set(tags)\n",
    "len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'NOUN': 28867,\n",
       "         '.': 11715,\n",
       "         'NUM': 3546,\n",
       "         'ADJ': 6397,\n",
       "         'VERB': 13564,\n",
       "         'DET': 8725,\n",
       "         'ADP': 9857,\n",
       "         'CONJ': 2265,\n",
       "         'X': 6613,\n",
       "         'ADV': 3171,\n",
       "         'PRT': 3219,\n",
       "         'PRON': 2737})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the most common_tags \n",
    "from collections import Counter\n",
    "tag_counts=Counter(tags)\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the most common tags is Noun , So lets assign tag Noun to unknown words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified viterbi algo which will assign tag NOUN to unknown words\n",
    "\n",
    "def Viterbi_POS(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1] \n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        if pmax!=0.0:\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)]\n",
    "        else:\n",
    "            # assigning the most occuring POS to unknown words\n",
    "            state_max='NOUN'\n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NOUN'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NOUN'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NOUN'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'NOUN'), (\"'s\", 'PRT'), ('firehose', 'NOUN'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'NOUN'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'NOUN'), ('.', '.'), ('The', 'DET'), ('2018', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'NOUN'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'NOUN'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'NOUN'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# lets check on test sample sentences \n",
    "tagged_seq_POS=Viterbi_POS(words)\n",
    "print(tagged_seq_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can see that for unknow words it has tagged the most occuring tag which is NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let apply rule based tagging and modify viterbi algo\n",
    "# specify patterns for tagging\n",
    "# example from the NLTK book\n",
    "patterns = [\n",
    "    (r'(.*ing|.*ed)$', 'VERB'),              \n",
    "    (r'\\d+','NUM'),\n",
    "    (r'.*ity$','ADV'),\n",
    "    (r'.*', 'NOUN'),\n",
    "    \n",
    "]\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "# help(regexp_tagger)\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "# lexicon backed up by the rule-based tagger\n",
    "lexicon_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will apply rule based modification to viterbi algo\n",
    "\n",
    "def Viterbi_rule_based(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1] \n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        if pmax!=0.0:\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)]\n",
    "        else:\n",
    "            # assigning the unknown tag through rules based \n",
    "#             print(lexicon_tagger.tag([word]))\n",
    "            state_max=lexicon_tagger.tag([word])[0][1]\n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NUM'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NUM'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NUM'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'NOUN'), (\"'s\", 'PRT'), ('firehose', 'NOUN'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'VERB'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'ADV'), ('.', '.'), ('The', 'DET'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'VERB'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'VERB'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# lets check on test sample sentences \n",
    "tagged_seq_rule_baes=Viterbi_rule_based(words)\n",
    "print(tagged_seq_rule_baes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now based on the rules many words are being tagged correctly like\n",
    "Android , OS , Google , 2013 , personality and many more unknow tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Despite of course being a very bad estimate of the true probability, zero-probabilities like this will also result in that\n",
    "all possible state-sequences for an observation sequence containing an unknown\n",
    "word will have probability 0 and therefore we cannot choose between them.\n",
    "So for unknows words we will assign emission probability to 0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified viterbi algo with laplace smoothing\n",
    "# Viterbi Heuristic\n",
    "def Viterbi_laplace(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]  or 0.001\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "#         print(\"pmax {0} for word {1}\".format(pmax,word))\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'NOUN'), ('operating', '.'), ('system', 'DET'), ('developed', 'NOUN'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'NOUN'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NOUN'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'DET'), ('since', 'NOUN'), ('2013', 'NOUN'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NOUN'), ('that', 'ADP'), ('gave', 'NOUN'), ('Google', 'NOUN'), ('access', '.'), ('to', 'PRT'), ('Twitter', 'VERB'), (\"'s\", 'PRT'), ('firehose', 'VERB'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', '.'), ('and', 'CONJ'), ('social', 'NOUN'), ('networking', '.'), ('service', 'DET'), ('on', 'NOUN'), ('which', 'DET'), ('users', 'ADJ'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'NOUN'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.'), ('Before', 'NOUN'), ('entering', 'NOUN'), ('politics', '.'), (',', '.'), ('Donald', 'DET'), ('Trump', 'ADJ'), ('was', 'NOUN'), ('a', 'DET'), ('domineering', 'NOUN'), ('businessman', '.'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'ADJ'), ('personality', 'NOUN'), ('.', '.'), ('The', 'DET'), ('2018', 'NOUN'), ('FIFA', 'NOUN'), ('World', '.'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', '.'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'NOUN'), ('football', '.'), ('tournament', 'NOUN'), ('contested', 'NOUN'), ('once', 'NOUN'), ('every', 'NOUN'), ('four', 'NOUN'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'DET'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'NOUN'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'DET'), ('.', '.'), ('Show', 'DET'), ('me', 'NOUN'), ('the', 'DET'), ('cheapest', 'NOUN'), ('round', '.'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'DET'), ('to', 'NOUN'), ('Atlanta', '.'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'X'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'VERB'), ('.', '.'), ('Show', 'DET'), ('me', 'NOUN'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'ADJ'), ('leaving', 'NOUN'), ('Atlanta', '.'), ('at', 'ADP'), ('about', 'NOUN'), ('3', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'ADJ'), ('and', 'CONJ'), ('arriving', 'NOUN'), ('in', 'ADP'), ('San', 'DET'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'NOUN'), ('social', 'NOUN'), ('media', '.'), ('users', 'DET'), ('to', 'NOUN'), ('experience', '.'), ('the', 'DET'), ('launch', 'ADJ'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# lets check on test sample sentences \n",
    "tagged_seq_laplace=Viterbi_laplace(words)\n",
    "print(tagged_seq_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214285714285714"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets evaluate tagging accuracy for vanila viterbi algo\n",
    "tagged_seq_vanila = Viterbi(test_tagged_words)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq_vanila, test_run_base) if i == j] \n",
    "\n",
    "accuracy_1 = len(check)/len(tagged_seq_vanila)\n",
    "accuracy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets evaluate tagging accuracy for modified viterbi with POS tagging\n",
    "tagged_seq_pos = Viterbi_POS(test_tagged_words)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq_pos, test_run_base) if i == j] \n",
    "\n",
    "accuracy_2 = len(check)/len(tagged_seq_pos)\n",
    "accuracy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857142857142858"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets evaluate tagging accuracy for modified viterbi with rule based tagging\n",
    "tagged_seq_rule_based = Viterbi_rule_based(test_tagged_words)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq_rule_based, test_run_base) if i == j] \n",
    "\n",
    "accuracy_3 = len(check)/len(tagged_seq_rule_based)\n",
    "accuracy_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6785714285714286"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets evaluate tagging accuracy for modified viterbi algo with laplace smoothing\n",
    "tagged_seq_laplace = Viterbi_laplace(test_tagged_words)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq_laplace, test_run_base) if i == j] \n",
    "\n",
    "accuracy_4 = len(check)/len(tagged_seq_laplace)\n",
    "accuracy_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanila viterbi algo tagging accuracy - 0.9214285714285714 <br>\n",
    "modified viterbi algo with POS tagging - 0.95 <br>\n",
    "modified vierbi algo with rule based tagging - 0.9857142857142858 <br>\n",
    "modified tagging with laplace smoothing - 0.6785714285714286 <br>\n",
    " It is clear that modified vierbi algo with rule based tagging has best accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is pos tagging by vanila viterbi algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('Android', '.'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', '.'), ('.', '.'), ('Android', '.'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', '.'), ('worldwide', '.'), ('on', 'ADP'), ('smartphones', '.'), ('since', 'ADP'), ('2011', '.'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', '.'), ('.', '.'),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is pos tagging by modified viterbi algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NUM'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NUM'),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare that unknow words like Android , Google ,2013 , smartphones, OS words have been tagged correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
